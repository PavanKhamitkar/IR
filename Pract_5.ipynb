{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cfaa7c7-9b05-47d1-a2a7-04eabe75f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590afd67-dec2-49ca-90da-5c285e7fbc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape a page for product info\n",
    "def scrape_page(url):\n",
    "    # Send a GET request to the page\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page: {url}\")\n",
    "        return []\n",
    "\n",
    "    # Parse the page content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all product elements on the page\n",
    "    product_items = soup.find_all('article', class_='product_pod')\n",
    "    \n",
    "    product_list = []\n",
    "    \n",
    "    # Extract relevant data from each product\n",
    "    for item in product_items:\n",
    "        title = item.h3.a['title']  # Extract the title from the <a> tag in <h3>\n",
    "        price = item.find('p', class_='price_color').text  # Extract the price\n",
    "        product_link = item.h3.a['href']  # Extract the link to the product page\n",
    "\n",
    "        # Build the full URL (if the link is relative)\n",
    "        full_product_link = f\"http://books.toscrape.com/catalogue/{product_link}\"\n",
    "\n",
    "        # Add to the product list\n",
    "        product_list.append({\n",
    "            'title': title,\n",
    "            'price': price,\n",
    "            'link': full_product_link\n",
    "        })\n",
    "\n",
    "    return product_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8caf0289-fd25-4b79-89ee-327ad7bb3a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape multiple pages\n",
    "def scrape_website(base_url, num_pages):\n",
    "    all_products = []\n",
    "\n",
    "    for page_num in range(1, num_pages + 1):\n",
    "        # Construct the URL for each page\n",
    "        url = f\"{base_url}/catalogue/page-{page_num}.html\"\n",
    "        print(f\"Scraping page: {url}\")\n",
    "        \n",
    "        # Scrape the current page\n",
    "        products_on_page = scrape_page(url)\n",
    "        \n",
    "        if products_on_page:\n",
    "            all_products.extend(products_on_page)\n",
    "        else:\n",
    "            print(f\"No products found on page: {page_num}\")\n",
    "    \n",
    "    return all_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eac886c-f9f1-40f2-8e12-a397ef41825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save data into a CSV file\n",
    "def save_to_csv(products, filename):\n",
    "    # Define the CSV column headers\n",
    "    csv_columns = ['title', 'price', 'link']\n",
    "    \n",
    "    # Open the file in write mode\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=csv_columns)\n",
    "        \n",
    "        # Write the header\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write the product data\n",
    "        for product in products:\n",
    "            writer.writerow(product)\n",
    "    \n",
    "    print(f\"Data saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461d74f6-e9de-4b4d-8d4f-7f501f8869b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "base_url = 'http://books.toscrape.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af6487e8-5383-470b-b56f-c659feae95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pages_to_scrape = 4  # Number of pages to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd7a905f-a90b-4d51-867f-7c140954bd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page: http://books.toscrape.com/catalogue/page-1.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-2.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-3.html\n",
      "Scraping page: http://books.toscrape.com/catalogue/page-4.html\n"
     ]
    }
   ],
   "source": [
    "# Scrape the e-commerce site\n",
    "products = scrape_website(base_url, num_pages_to_scrape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd48b4d7-17d5-4e25-806e-e838ba7dddf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to scraped_products.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the scraped data to a CSV file\n",
    "csv_filename = 'scraped_products.csv'\n",
    "save_to_csv(products, csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb00267-873e-441e-9eed-ed264bc0bce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ee37b-aaab-481c-92a8-05935ecaaf9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
